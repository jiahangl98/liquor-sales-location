[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "On this about page, you might want to add more information about yourself, the project, or course. Any helpful context could go here!\nMy name is Nick Hand, the instructor for the course. You can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2023.\nAdipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate.",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Python code blocks"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/sales (2).html",
    "href": "analysis/sales (2).html",
    "title": "3.Data cleaning and overview",
    "section": "",
    "text": "#1. Project Background\nThe liquor industry plays a significant role in the regional economy. As reported by the Distilled Spirits Council of the United States, the market share of liquor revenue has steadily increased, rising from 28.7% in 2000 to 42.1% in 2022, surpassing beer for the first time ever (Distilled Spirits Council of the United States, 2023). Des Moines, one of the major cities in State Iowa, has a dynamic and evolving liquor market. According to State of Iowa, the liquors sales in Des Maine shows an upward trend since 2012(State of Iowa 2023). Understanding the intricacies of liquor sales is crucial for businesses aiming to thrive in this sector. The focus of this project was to aid small business owners aiming to establish liquor stores in Iowa. By examining the liquor sales dataset from 2012 to 2016 provided by the State of Iowa, the project aimed to recommend ideal locations and product categories for these new businesses in Des Moines.\n#2.Import relevant packages and dataset\nfrom google.colab import drive  #I put this code into a Python file called Google_Connection and copied it to my Google Drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n#import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport string\nimport seaborn as sns\nimport altair as alt\nimport scipy.stats as stats\npd.set_option('display.max_columns', 10, 'display.max_rows', 10,'display.float_format', lambda x: '%.2f' %x)"
  },
  {
    "objectID": "analysis/sales (2).html#data-pattern",
    "href": "analysis/sales (2).html#data-pattern",
    "title": "3.Data cleaning and overview",
    "section": "Data Pattern",
    "text": "Data Pattern\n\n# Plot histograms for all variables\nax = df_cleaned.hist(figsize=(20, 20))\n# Setting y-scale to log for each subplot\nfor axis in ax.flatten():\n    axis.set_yscale('log')\nplt.show()\n\n\n\n\n\n\n\n\nAs is shown in the graphs, the variables considered are not normally distributed. Primarily, the ‘State bottle retail’ data indicates that most bottles are priced below 500 dollars. However, a distinct segment exists for luxury products, with some bottles priced over 8,000 dollars. Analyzing key categories such as bottles sold, volumes sold, and sales amount reveals that most transactions are below 25,000 dollars. Its also observed that typical purchases involve fewer than 1,000 bottles per transaction. Nevertheless, the data shows exceptions where some transactions involve the purchase of more than 12,500 bottles, highlighting a diverse range of buying behaviors. The high standard deviation of the ‘Sales amount’ variable also suggests the wide range of buying behaviors among consumers."
  },
  {
    "objectID": "analysis/sales (2).html#correlation-mapping",
    "href": "analysis/sales (2).html#correlation-mapping",
    "title": "3.Data cleaning and overview",
    "section": "Correlation Mapping",
    "text": "Correlation Mapping\n\n#Correlation Analysis\ncorr_matrix = df_cleaned.corr()\nplt.figure(figsize=(10, 8))  # Size of the figure\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.show()\n\nFutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_matrix = df_cleaned.corr()\n\n\n\n\n\n\n\n\n\nThe correlation matrix also helps us understand the values and their relationships. For example, the sales amount has strong relationship with the number of bottles sold and volumes sold."
  },
  {
    "objectID": "analysis/sales (2).html#data-cleaning",
    "href": "analysis/sales (2).html#data-cleaning",
    "title": "3.Data cleaning and overview",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n\nCode\n#createvodkagin\ndf_cleaned['Categorynew'] = df_cleaned['Categoryname'].str.split()\n\ndef categorize_description(Categorynew):\n    if 'GINS' in Categorynew:\n        return 1\n    elif 'SCHNAPPS' in Categorynew:\n        return 1\n    elif 'TEQUILA' in Categorynew:\n        return 2\n    elif 'RUM' in Categorynew:\n        return 3\n    elif 'WHISKEY' in Categorynew:\n        return 4\n    elif 'WHISKIES' in Categorynew:\n        return 4\n    elif 'VODKA' in Categorynew:\n        return 5\n    elif 'VODKAS' in Categorynew:\n        return 5\n    elif 'BRANDIES' in Categorynew:\n        return 6\n    elif 'LIQUEURS' in Categorynew:\n        return 7\n    else:\n        return 0  # or another value to signify no match\n\ndf_cleaned['Categorytype'] = df_cleaned['Categorynew'].apply(categorize_description)\n\n\n\n\nCode\ndf_cleaned\nconditions = [\n    df_cleaned['Categorytype'] == 0,\n    df_cleaned['Categorytype'] == 1,\n    df_cleaned['Categorytype'] == 2,\n    df_cleaned['Categorytype'] == 3,\n    df_cleaned['Categorytype'] == 4,\n    df_cleaned['Categorytype'] == 5,\n    df_cleaned['Categorytype'] == 6,\n    df_cleaned['Categorytype'] == 7,\n]\nchoices = ['OTHERS', 'GINS', 'TEQUILA','RUM','WHISKEY','VODKA','BRANDIES','LIQUEURS']\ndf_cleaned['categorytype'] = np.select(conditions, choices, default='other_value')\n\n\n\n\nCode\n#Popular drink\ndf_sales = df_cleaned[['categorytype', 'Salesamount','Date']]\n#df_sales['Year'] = df_sales['Date'].dt.year\n\n#Add date parts to the dataframe for future use in the analysis\ndf_sales = df_sales.assign(orderyear = df_sales['Date'].dt.year)\ndf_sales = df_sales.assign(orderquarter = df_sales['Date'].dt.quarter)\ndf_sales = df_sales.assign(ordermonth = df_sales['Date'].dt.month)\ndf_sales = df_sales.assign(orderday = df_sales['Date'].dt.day)\ndf_sales = df_sales.assign(daynumber = df_sales['orderyear']+df_sales['ordermonth']+df_sales['orderday'])\n\n#Determine quarterly sales.  Notice 2010 and 2014 are incomplete years.\ndf_sales=df_sales.groupby(['orderyear','orderquarter','categorytype']).agg({'Salesamount':'sum'}).reset_index()\n\n\n\n\nCode\n# Resetting index and creating a pivot table\npivot_df = df_sales.pivot(index=['orderyear', 'orderquarter'], columns='categorytype', values='Salesamount')\n#Remove 2016 Q2 due to insufficient data\npivot_df = pivot_df.iloc[:-1]\n\n\n\n\nCode\nimport altair as alt\nimport pandas as pd\n\n# Assuming melted_df is your melted DataFrame\n# Melt the DataFrame for Altair\nmelted_df = pivot_df.reset_index().melt(id_vars=['orderyear', 'orderquarter'], var_name='categorytype', value_name='Salesamount')\n# Create a calculated field for the x-axis labels\nmelted_df['year_quarter'] = melted_df['orderyear'].astype(str) + ' Q' + melted_df['orderquarter'].astype(str)\nmelted_df\n\n\n\n  \n    \n\n\n\n\n\n\norderyear\norderquarter\ncategorytype\nSalesamount\nyear_quarter\n\n\n\n\n0\n2012\n1\nBRANDIES\n122351.31\n2012 Q1\n\n\n1\n2012\n2\nBRANDIES\n130282.47\n2012 Q2\n\n\n2\n2012\n3\nBRANDIES\n150196.88\n2012 Q3\n\n\n3\n2012\n4\nBRANDIES\n176500.21\n2012 Q4\n\n\n4\n2013\n1\nBRANDIES\n160678.18\n2013 Q1\n\n\n...\n...\n...\n...\n...\n...\n\n\n131\n2015\n1\nWHISKEY\n553685.27\n2015 Q1\n\n\n132\n2015\n2\nWHISKEY\n619418.49\n2015 Q2\n\n\n133\n2015\n3\nWHISKEY\n595603.16\n2015 Q3\n\n\n134\n2015\n4\nWHISKEY\n663841.30\n2015 Q4\n\n\n135\n2016\n1\nWHISKEY\n598447.97\n2016 Q1\n\n\n\n\n136 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nCode\n# Create Altair chart\nchart = alt.Chart(melted_df).mark_line().encode(\n    x=alt.X('year_quarter:O', axis=alt.Axis(title='Year and Quarter')),\n    y=alt.Y('Salesamount:Q', axis=alt.Axis(title='Sales')),\n    color='categorytype:N',\ntooltip=[\"Salesamount\", \"categorytype\"],).properties(\n    width=800,\n    height=400,\n    title='Sales Trend Over Years for Different Types of Alcohol'\n)\n\n\n\n# Show the Altair chart\nchart\n\n\n\n\n\n\nFrom the graph, it is evident that Vodka and Whiskey stand out as the leading choices among alcoholic products. On the other hand, beverages like Rum,Gin, and Liqueurs exhibit comparatively lower popularity. These findings imply that business owners looking to import alcohol might benefit from prioritizing Whiskey and Vodka, given their higher demand.\n\n#5. Machine Learning\n#5.1 Random forest to identify important features\n\n\nCode\n#Clean the data\ndfperdict=dffinal\ndfperdict.drop(['Categoryname','Year'], axis=1, inplace=True)\nprint(dfperdict.columns)\n\n\nKeyError: \"['Categoryname', 'Year'] not found in axis\"\n\n\n\n#Import modeling package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCode\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n# Filter out data for Y for random forest\ndfsalesa=dffinal['Salesamount']\n\n\n\n\nCode\n# Convert categorical variables (strings) to numeric\nlabel_encoder = LabelEncoder()\ndfperdict['Invoice/Item Number'] = label_encoder.fit_transform(dfperdict['Invoice/Item Number'])\ndfperdict['Date'] = label_encoder.fit_transform(dfperdict['Date'])\ndfperdict['Store Name'] = label_encoder.fit_transform(dfperdict['Store Name'])\ndfperdict['Vendor Name'] = label_encoder.fit_transform(dfperdict['Vendor Name'])\ndfperdict['Item Description'] = label_encoder.fit_transform(dfperdict['Item Description'])\ndfperdict['Address'] = label_encoder.fit_transform(dfperdict['Address'])\ndfperdict['Category'] = label_encoder.fit_transform(dfperdict['Category'])\ndfperdict['City'] = label_encoder.fit_transform(dfperdict['City'])\ndfperdict['Store Location'] = label_encoder.fit_transform(dfperdict['Store Location'])\ndfperdict['County'] = label_encoder.fit_transform(dfperdict['County'])\ndfperdict['Category Name'] = label_encoder.fit_transform(dfperdict['Category Name'])\ndfperdict['Store Number'] = label_encoder.fit_transform(dfperdict['Store Number'])\n#dfperdict['Volume Sold (Liters)'] = label_encoder.fit_transform(dfperdict['Volume Sold (Liters)'])\n#dfperdict['VolumeSoldGallons'] = label_encoder.fit_transform(dfperdict['VolumeSoldGallons'])\n#dfperdict['Bottle Volume (ml)'] = label_encoder.fit_transform(dfperdict['Bottle Volume (ml)'])\n\n\n\n\nCode\ndfperdict.dtypes\n\n\nCounty Number       float64\nCategory              int64\nVendor Number       float64\nItem Number         float64\nPack                float64\n                     ...   \nStore Location        int64\nCounty                int64\nCategory Name         int64\nVendor Name           int64\nItem Description      int64\nLength: 24, dtype: object\n\n\n\n# Set feature and target\n# Features\nX = dfperdict.drop('Salesamount', axis=1).values\n# Target variable\ny = dfsalesa.values\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize the Random Forest Regressor model\nforest_model = RandomForestRegressor(n_estimators=10, random_state=42)\n\n# Train the model\nforest_model.fit(X_train, y_train)\nforest_model.score(X_train, y_train)\n\n# Predict on the test set\ny_pred = forest_model.predict(X_test)\n\n# Feature importances\nfeature_importances = forest_model.feature_importances_\nindices = np.argsort(feature_importances)[::-1]\n\n#featlabels\nfeat_labels = dfperdict.drop('Salesamount', axis=1).columns\n\n\nfeature_importances\n\narray([0.00000000e+00, 5.90724854e-07, 8.32376662e-07, 1.06195112e-06,\n       0.00000000e+00, 6.71598516e-07, 3.97657930e-01, 1.25664616e-05,\n       5.56143829e-01, 4.61330210e-02, 3.81519594e-05, 1.14753471e-06,\n       1.32019869e-06, 2.93377546e-07, 4.09545409e-07, 3.88973112e-07,\n       0.00000000e+00, 4.69119410e-08, 3.26437138e-07, 0.00000000e+00,\n       5.63584678e-06, 1.12234689e-06, 6.54219329e-07])\n\n\n\n\nCode\nindices\n\n\narray([ 8,  6,  9, 10,  7, 20, 12, 11, 21,  3,  2,  5, 22,  1, 14, 15, 18,\n       13, 17, 16,  4, 19,  0])\n\n\n\n\nCode\nfeat_labels\n\n\nIndex(['County Number', 'Category', 'Vendor Number', 'Item Number', 'Pack',\n       'Bottle Volume (ml)', 'State Bottle Retail', 'Volume Sold (Liters)',\n       'BottlesSold', 'StateBottleCost', 'VolumeSoldGallons',\n       'Invoice/Item Number', 'Date', 'Store Number', 'Store Name', 'Address',\n       'City', 'Zip Code', 'Store Location', 'County', 'Category Name',\n       'Vendor Name', 'Item Description'],\n      dtype='object')\n\n\n\n\nCode\n# Corrected loop using feature_importances and indices\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], feature_importances[indices[f]]))\n\n\n 1) BottlesSold                    0.556144\n 2) State Bottle Retail            0.397658\n 3) StateBottleCost                0.046133\n 4) VolumeSoldGallons              0.000038\n 5) Volume Sold (Liters)           0.000013\n 6) Category Name                  0.000006\n 7) Date                           0.000001\n 8) Invoice/Item Number            0.000001\n 9) Vendor Name                    0.000001\n10) Item Number                    0.000001\n11) Vendor Number                  0.000001\n12) Bottle Volume (ml)             0.000001\n13) Item Description               0.000001\n14) Category                       0.000001\n15) Store Name                     0.000000\n16) Address                        0.000000\n17) Store Location                 0.000000\n18) Store Number                   0.000000\n19) Zip Code                       0.000000\n20) City                           0.000000\n21) Pack                           0.000000\n22) County                         0.000000\n23) County Number                  0.000000\n\n\n\n# Plot feature importances\nplt.figure(figsize=(12, 8))\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')\nplt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the number of bottles sold, state bottle retail price and cost price have the highest influence on the salesamount."
  },
  {
    "objectID": "analysis/sales (2).html#identify-the-cluster-number",
    "href": "analysis/sales (2).html#identify-the-cluster-number",
    "title": "3.Data cleaning and overview",
    "section": "Identify the cluster number",
    "text": "Identify the cluster number\nThen, K-Means Cluster is implemented to classify the alcohol products to see the sales pattern of the different categories. Based on the random forest result, ‘State Bottle Cost’, ‘State Bottle Retail’,and‘Volume Sold (Gallons)’ are selected for this K-means model as they are key representatives of the popularity and consumption levels of the alcoholic products.\n\nfrom sklearn.cluster import KMeans\n\n\n# Selecting relevant columns\ndata_scaled = dfstand[['State Bottle Retail', 'Volume Sold (Liters)','StateBottleCost']]\n\n\n\nCode\n# Determine the appropriate number of clusters K using the elbow method\ndistortions = []\nK = range(1, 11)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(data_scaled)\n    distortions.append(kmeans.inertia_)\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\n# Plotting the Elbow Method graph\nimport matplotlib.pyplot as plt\nplt.plot(K, distortions, 'bx-')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()\n\n\n\n\n\n\n\n\nBased on the result from the elbow method, k=3 is chosen for this analysis to ensure a balance between the models complexity and the clarity of the categorization."
  },
  {
    "objectID": "analysis/sales (2).html#visualization",
    "href": "analysis/sales (2).html#visualization",
    "title": "3.Data cleaning and overview",
    "section": "Visualization",
    "text": "Visualization\n\n# Assuming k is chosen as 3 for this example\nkmeans = KMeans(n_clusters=3, random_state=0).fit(data_scaled)\n\n# Adding cluster labels to your original dataframe\ndf_cleaned['cluster'] = kmeans.labels_\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\n# Scatter plot of data points with color-coded clusters\nplt.scatter(data_scaled.iloc[:, 0], data_scaled.iloc[:, 1], c=labels, cmap='viridis', s=50)\n\n# Mark cluster centers with 'X' markers\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, color='red', label='Centroids')\nplt.title(f'K-means Clustering (K={3})')\nplt.legend()\nplt.show()\n\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nComparison of Clusters\n\n\nCode\n# Group by cluster and see mean values\nclustered_data = df_cleaned.groupby('cluster').mean()\nclustered_data.drop(['County Number', 'Vendor Number', 'Category','Item Number','Year'], axis=1, inplace=True)\n\n\nFutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  clustered_data = df_cleaned.groupby('cluster').mean()\n\n\n\nclustered_data\n\n\n  \n    \n\n\n\n\n\n\nPack\nBottle Volume (ml)\nState Bottle Retail\nVolume Sold (Liters)\nSalesamount\nBottlesSold\nStateBottleCost\nVolumeSoldGallons\nCategorytype\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n12.00\n801.57\n9.58\n10.02\n119.53\n12.56\n6.37\n2.65\n3.85\n\n\n1\n12.00\n783.39\n22.38\n5.77\n157.92\n7.20\n14.90\n1.53\n4.20\n\n\n2\n12.00\n684.27\n11.32\n2.02\n36.21\n3.20\n7.53\n0.53\n3.93\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nTotal sales for each cluster across 2012-2016\n\nclustersum = df_cleaned.groupby('cluster')['Salesamount'].sum()\nclustersum\n\ncluster\n0   21062764.82\n1   12571780.86\n2    5805160.62\nName: Salesamount, dtype: float64\n\n\nAfter performing the algorithm, the dataset is categorized into three distinct clusters. These clusters were analyzed based on the average value of several key variables: SalesAmount, Bottles Sold, State Bottle Retail, and Volume Sold (Gallons) , and the city averages. Consequently, descriptive labels were assigned to each cluster: Popular Bottles, Less Popular Bottles, and High End. The High End cluster,cluster1, is characterized by ‘State Bottle Cost’ value, which notably exceeds the average of $8.45. This higher cost is indicative of its premium positioning. Meanwhile, Cluster0, the most popular Bottles, demonstrates significantly higher metrics across SalesAmount, Bottles Sold, and Volume Sold categories when compared to the average city levels. The remaining cluster,cluster2, Less Popular Bottles, encompasses products that do not align with the high sales figures of the Popular Bottles or the premium pricing of the High End cluster."
  },
  {
    "objectID": "analysis/sales (2).html#import-data",
    "href": "analysis/sales (2).html#import-data",
    "title": "3.Data cleaning and overview",
    "section": "Import data",
    "text": "Import data\nThe author employs the ‘tidycensus’ function in R to retrieve census data for Polk County, encompassing both total population and age distribution at the block level. Subsequently, ArcGIS is utilized to clip the shapefile to the Des Moines boundary. Lastly, GIS tools are applied to compute the percentage of the population over 21.\n\n#Read data\nPolkcounty = gp.read_file('/content/drive/MyDrive/MSSP6070/Assignments/A4/CensusData2.shp')\n\n\nPolkcounty\nprint(Polkcounty.columns)\n\nIndex(['STATEFP', 'COUNTYF', 'TRACTCE', 'BLKGRPC', 'GEOID', 'NAMELSA', 'MTFCC',\n       'FUNCSTA', 'ALAND', 'AWATER', 'INTPTLA', 'INTPTLO', 'NAME', 'B19013_',\n       'B01001_007', 'B01001_008', 'B01001_009', 'B01001_010', 'B01001_011',\n       'B01001_012', 'B01001_013', 'B01001_014', 'B01001_015', 'B01001_016',\n       'B01001_017', 'POPULAT', 'p_POPUL', 'Mdnhshl', 'geometry'],\n      dtype='object')"
  },
  {
    "objectID": "analysis/sales (2).html#calculate-the-population-age-index",
    "href": "analysis/sales (2).html#calculate-the-population-age-index",
    "title": "3.Data cleaning and overview",
    "section": "Calculate the population age index",
    "text": "Calculate the population age index\n\n# Function to apply\ndef categorize_value(value):\n    if value &gt; 0.379353:\n        return 4\n    elif value &gt; 0.311703:\n        return 3\n    elif value &gt; 0.252007:\n        return 2\n    elif value &gt; 0.144646:\n        return 1  # You can assign a different value for values below 1.5 if needed\n    else:\n        return 0\n\n# Apply the function to create a new column\nPolkcounty['populationindex'] = Polkcounty['p_POPUL'].apply(categorize_value)"
  },
  {
    "objectID": "analysis/sales (2).html#calculate-the-income-index",
    "href": "analysis/sales (2).html#calculate-the-income-index",
    "title": "3.Data cleaning and overview",
    "section": "Calculate the income index",
    "text": "Calculate the income index\n\n# Function to apply\ndef categorize_income(income):\n    if income &gt; 88482:\n        return 4\n    elif income &gt; 60417:\n        return 3\n    elif income &gt; 42700:\n        return 2\n    elif income &gt; 24485:\n        return 1  # You can assign a different value for values below 1.5 if needed\n    else:\n        return 0\n\n# Apply the function to create a new column\nPolkcounty['incomeindex'] = Polkcounty['Mdnhshl'].apply(categorize_income)"
  },
  {
    "objectID": "analysis/sales (2).html#calculate-the-demographic-index-with-the-sum-of-the-income-and-population-age-index",
    "href": "analysis/sales (2).html#calculate-the-demographic-index-with-the-sum-of-the-income-and-population-age-index",
    "title": "3.Data cleaning and overview",
    "section": "Calculate the demographic index with the sum of the income and population age index",
    "text": "Calculate the demographic index with the sum of the income and population age index\n\n#Get the sum of the other two indexs\nPolkcounty['demographicindex'] = Polkcounty['incomeindex'] + Polkcounty['populationindex']"
  },
  {
    "objectID": "analysis/sales (2).html#write-the-new-shp-file-for-gis-analysis",
    "href": "analysis/sales (2).html#write-the-new-shp-file-for-gis-analysis",
    "title": "3.Data cleaning and overview",
    "section": "Write the new shp file for GIS analysis",
    "text": "Write the new shp file for GIS analysis\n\nPolkcounty.to_file(\"/content/drive/MyDrive/MSSP6070/Assignments/A4/desmainincome.shp\")\n\nUserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n  Polkcounty.to_file(\"/content/drive/MyDrive/MSSP6070/Assignments/A4/desmainincome.shp\")\nWARNING:fiona._env:Normalized/laundered field name: 'incomeindex' to 'incomeinde'\nWARNING:fiona._env:Normalized/laundered field name: 'populationindex' to 'population'\nWARNING:fiona._env:Normalized/laundered field name: 'demographicindex' to 'demographi'\nWARNING:fiona._env:Value 121734842 of field ALAND of feature 14 not successfully written. Possibly due to too larger number with respect to field width\nWARNING:fiona._env:Value 160958740 of field ALAND of feature 36 not successfully written. Possibly due to too larger number with respect to field width"
  },
  {
    "objectID": "analysis/sales (2).html#extract-the-shop-coordinates",
    "href": "analysis/sales (2).html#extract-the-shop-coordinates",
    "title": "3.Data cleaning and overview",
    "section": "Extract the shop coordinates",
    "text": "Extract the shop coordinates\n\nshoplocation = df_cleaned[['Store Location', 'Salesamount', 'Store Name','cluster','Store Number']]\n\n\nimport re\n# Define a regular expression pattern to extract latitude and longitude\npattern = r'\\(([\\d.-]+),\\s+([\\d.-]+)\\)'\n\n# Use str.extract() to create new 'Latitude' and 'Longitude' columns\nshoplocation[['Longitude', 'Latitude']] = shoplocation['Store Location'].str.extract(pattern)\n\n# Convert the columns to numeric\nshoplocation['Longitude'] = pd.to_numeric(shoplocation['Longitude'])\nshoplocation['Latitude'] = pd.to_numeric(shoplocation['Latitude'])\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  shoplocation[['Longitude', 'Latitude']] = shoplocation['Store Location'].str.extract(pattern)\n&lt;ipython-input-75-39c934a9440b&gt;:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  shoplocation[['Longitude', 'Latitude']] = shoplocation['Store Location'].str.extract(pattern)\n&lt;ipython-input-75-39c934a9440b&gt;:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  shoplocation['Longitude'] = pd.to_numeric(shoplocation['Longitude'])\n&lt;ipython-input-75-39c934a9440b&gt;:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  shoplocation['Latitude'] = pd.to_numeric(shoplocation['Latitude'])\n\n\n\n# Assuming you have a DataFrame named shoplocation\nshoplocationsum1 = shoplocation.groupby(['Store Number', 'Store Name', 'Latitude', 'Longitude'])['Salesamount'].sum().reset_index()\n\n\n#Write the file\nshoplocationsum1.to_csv(r\"/content/drive/MyDrive/MSSP6070/Assignments/A4/shoplocationsum1.csv\", index=False)"
  },
  {
    "objectID": "analysis/sales (2).html#add-a-basemap-and-visualize-shops-based-on-its-sales",
    "href": "analysis/sales (2).html#add-a-basemap-and-visualize-shops-based-on-its-sales",
    "title": "3.Data cleaning and overview",
    "section": "Add a basemap and visualize shops based on its sales",
    "text": "Add a basemap and visualize shops based on its sales\n\n# Plot the narrowed GeoDataFrame\nax = desmainetrimed.plot(figsize=(10, 10), color='white', edgecolor='black')\n\n# Plot the 'gdf' GeoDataFrame with 'Salesamount' coloring\ngdf.plot(ax=ax, column='Salesamount', cmap='viridis', legend=True, markersize=100, label='Stores', marker='o')\n\n# Add labels to the points\nfor x, y, label in zip(gdf.geometry.x, gdf.geometry.y,gdf['Salesamount']):\n    ax.text(x, y, label, fontsize=5, ha='center', va='center')\n\nplt.title('Store Locations visualization based on salesamount')\nplt.legend()\n\n\n\n\n\n\n\n\nThe location dot shows that liquor shops are located along the block junctions and street interfaces.This placement is often influenced by factors such as accessibility, visibility, and zoning regulations.We can also see that they are conecentrated along the inner city.\n\n\nCode\nimport geodatasets\n!pip install folium\n\n\nRequirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: branca&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.7.0)\nRequirement already satisfied: jinja2&gt;=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.23.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.31.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2&gt;=2.9-&gt;folium) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;folium) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;folium) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;folium) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;folium) (2023.11.17)"
  },
  {
    "objectID": "analysis/sales (2).html#plot-an-interacctive-page-showing-the-houshold-income-at-block-level",
    "href": "analysis/sales (2).html#plot-an-interacctive-page-showing-the-houshold-income-at-block-level",
    "title": "3.Data cleaning and overview",
    "section": "Plot an interacctive page showing the houshold income at block level",
    "text": "Plot an interacctive page showing the houshold income at block level\nThis interface provides a visualization of the income profile of the city at the block level. Blocks with darker colors indicate a lower median household income and lower ratio of popualtion over21, offering a quick and visually intuitive representation of economic disparities across different areas within the city.\n\nURL1=desmainetrimed.explore(\n    column=\"demographi\",  # Similar to plot(); specify the value column\n    cmap=\"viridis\",  # What color map do we want to use\n    #tiles=\"CartoDB positron\",  # What basemap tiles do we want to use?\n)\nURL1\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAfter comparing it with the liquor sales location, we can see that liquor atores are concentrated in blocks with relatively lower income level. There is potential to open business in relatively weathlier outskirts area.Also, the potential imported goods may be goods with higher retail price."
  }
]